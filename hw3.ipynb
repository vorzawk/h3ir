{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2018\n",
    "\n",
    "\n",
    "# Homework 3:  Embeddings + Recommenders\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Monday, April 9 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* There are two main learning objectives: (i) implement and evaluate a pre-cursor to modern word2vec embeddings; and (ii) implement, evaluate, and improve upon traditional collaborative filtering recommenders.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as UIN_hw#.ipynb. For example, this homework submission would be: YourUIN_hw3.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Thursday, April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imported libraries\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Word Embeddings (50 points)\n",
    "For this first part, we're going to implement a word embedding approach that is a bit simpler than word2vec. The key idea is to look at co-occurrences between center words and context words (somewhat like in word2vec) but without any pesky learning of model parameters.\n",
    "\n",
    "If you're interested in a deeper treatment of comparing count vs. learned embeddings, take a look at: [Donâ€™t count, predict! A systematic comparison of\n",
    "context-counting vs. context-predicting semantic vectors](\n",
    "http://www.aclweb.org/anthology/P14-1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Brown Corpus\n",
    "\n",
    "The dataset for this part is the (in)famous [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) that is a collection of text samples from a wide range of sources, with over one million unique words. Good for us, you can find the Brown corpus in nltk. *Make sure you have already installed nltk with something like: conda install nltk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/abhilash/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have it locally, you can load the dataset into your notebook. You can access the words using brown.words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'The', u'Fulton', u'County', u'Grand', u'Jury', ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset Pre-processing\n",
    "OK, now we need to do some basic pre-processing. For this part you should:\n",
    "\n",
    "* Remove stopwords and punctuation.\n",
    "* Make everything lowercase.\n",
    "\n",
    "Then, count how often each word occurs. We will define the 5,000 most  frequent words as your vocabulary (V). We will define the 1,000 most frequent words as our context (C). Include a print statement below to show the top-20 words after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import string\n",
    "\n",
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(fileids='english')\n",
    "sentence = 'The quick brown fox was too quick for the white fox brown quick \\\n",
    "fox'\n",
    "listWords = sentence.split()\n",
    "listWords = brown.words()\n",
    "brown_noStopWords = [w for w in listWords if w.lower() not in stop_words]\n",
    "\n",
    "# Remove punctuation\n",
    "import re\n",
    "brown_noPunctuation = [re.sub(r'[^\\w]','',w) for w in brown_noStopWords]\n",
    "# Remove the leftover empty strings, so that they don't get counted in the\n",
    "# window operation\n",
    "brown_noPunctuation = [w for w in brown_noPunctuation if w != ''] \n",
    "\n",
    "# stem the words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "brown_preprocessed = [ stemmer.stem(w) for w in brown_noPunctuation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one: 3480\n",
      "would: 2714\n",
      "said: 1961\n",
      "time: 1930\n",
      "year: 1659\n",
      "new: 1647\n",
      "could: 1601\n",
      "state: 1562\n",
      "like: 1535\n",
      "use: 1476\n",
      "may: 1419\n",
      "two: 1414\n",
      "first: 1361\n",
      "man: 1352\n",
      "even: 1319\n",
      "make: 1225\n",
      "work: 1178\n",
      "made: 1125\n",
      "day: 1088\n",
      "also: 1069\n"
     ]
    }
   ],
   "source": [
    "# The 5000 most common words are the vocabulary and the 1000 most common words\n",
    "# are the Context.\n",
    "dictWordCnt = {}\n",
    "for token in brown_preprocessed:\n",
    "    if token in dictWordCnt:\n",
    "        dictWordCnt[token] += 1\n",
    "    else:\n",
    "        dictWordCnt[token] = 1\n",
    "# sort the dictionary and return the keys in decreasing order of word frequency\n",
    "cnt = 0\n",
    "listSortedKeys = sorted(dictWordCnt, key=dictWordCnt.get, reverse=True)\n",
    "for w in listSortedKeys:\n",
    "    cnt += 1\n",
    "    print (w + ': {}'.format(dictWordCnt[w]))\n",
    "    if cnt >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictVocab2idx = {}\n",
    "dictContext2idx = {}\n",
    "dictIdx2Vocab = {}\n",
    "dictIdx2Context = {}\n",
    "for idx,token in enumerate(listSortedKeys[:5000]):\n",
    "    dictVocab2idx[token] = idx\n",
    "    dictIdx2Vocab[idx] = token\n",
    "    if idx < 1000:\n",
    "        dictIdx2Context[idx] = token\n",
    "        dictContext2idx[token] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building the Co-occurrence Matrix \n",
    "\n",
    "For each word in the vocabulary (w), we want to calculate how often context words from C appear in its surrounding window of size 4 (two words before and two words after).\n",
    "\n",
    "In other words, we need to define a co-occurrence matrix that has a dimension of |V|x|C| such that each cell (w,c) represents the number of times c occurs in a window around w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "CONTEXT_SIZE = 1000\n",
    "cooccurence_matrix = np.zeros(shape=(VOCAB_SIZE,CONTEXT_SIZE))\n",
    "for index,w in enumerate(brown_preprocessed[2:-2]):\n",
    "    actIndex = index + 2    # Actual index is 2 more than index\n",
    "    # Check if the word is in Vocab, then check the two words prior to it and\n",
    "    # the 2 words after it to see if they belong to the Context set. Increment\n",
    "    # the (w,c) entry if they do.\n",
    "    if w in dictVocab2idx:\n",
    "        window_words = (brown_preprocessed[actIndex-2],brown_preprocessed[actIndex-1],\n",
    "         brown_preprocessed[actIndex+1],brown_preprocessed[actIndex+2])\n",
    "        for c in window_words:\n",
    "            if c in dictContext2idx:\n",
    "                cooccurence_matrix[dictVocab2idx[w]][dictContext2idx[c]] += 1\n",
    "#print(cooccurence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Probability Distribution\n",
    "\n",
    "Using the co-occurrence matrix, we can compute the probability distribution Pr(c|w) of context word c around w as well as the overall probability distribution of each context word c with Pr(c).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwPDF = np.zeros(shape=(VOCAB_SIZE,CONTEXT_SIZE))\n",
    "for wIdx,cwArray in enumerate(cooccurence_matrix):\n",
    "    sumCw = sum(cwArray)\n",
    "    for cIdx,val in enumerate(cwArray):\n",
    "        cwPDF[wIdx][cIdx] = val/sumCw\n",
    "# print(cwPDF)\n",
    "sumC = np.sum(np.transpose(cooccurence_matrix),axis=1)\n",
    "cPDF = sumC/np.sum(cooccurence_matrix)\n",
    "# print(cPDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Embedding Representation\n",
    "\n",
    "Now you can represent each vocabulary word as a |C| dimensional vector using this equation:\n",
    "\n",
    "Vector(w)= max(0, log (Pr(c|w)/Pr(c)))\n",
    "\n",
    "This is a traditional approach called *pointwise mutual information* that pre-dates word2vec by some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwPDF += 0.0000001\n",
    "embedding = np.log(cwPDF/cPDF)\n",
    "embedding = np.maximum(0,embedding)\n",
    "# print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Analysis\n",
    "\n",
    "So now we have some embeddings for each word. But are they meaningful? For this part, you should:\n",
    "\n",
    "- First, cluster the vocabulary into 100 clusters using k-means. Look over the words in each cluster, can you see any relation beween words? Discuss your observations.\n",
    "\n",
    "- Second, for the top-20 most frequent words, find the nearest neighbors using cosine distance (1- cosine similarity). Do the findings make sense? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, random_state=0).fit(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'temperatur', u'fig', u'anod', u'16', u'ratio', u'fiber', u'arc', u'diamet', u'lb', u'cm', u'ft', u'mm']\n"
     ]
    }
   ],
   "source": [
    "clusterElemIndices = np.where(kmeans.labels_==20)[0]\n",
    "listClusterElems = [dictIdx2Vocab[index] for index in clusterElemIndices]\n",
    "print(listClusterElems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above words belong to the 20th cluster and it is clear that they represent measurements, the units like lb, cm, ft, mm and also the quantities like temperature, arc, diameter and ratio appear here. This makes sense because usually the quantities and their units of measurements occur together and are closely related.\n",
    "\n",
    "The above method represents each word by the words around it, so words which have similar contexts will end having similar representations and get grouped in the same cluster.\n",
    "\n",
    "There are also words like fig, anode and fiber in the same cluster which suggests that method is not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'one', u'would', u'said', u'time', u'year', u'new', u'could', u'state', u'like', u'use', u'may', u'two', u'first', u'man', u'even', u'make', u'work', u'made', u'day', u'also']\n",
      "[u'said', u'know', u'ask', u'tell', u'talk', u'Im']\n",
      "[u'year', u'month']\n",
      "[u'two', u'three']\n",
      "[u'day', u'week']\n"
     ]
    }
   ],
   "source": [
    "def compute_cosineSimilarity(a,b):\n",
    "    return np.dot(a,b)/ (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def compute_cosineDistance(u,v):\n",
    "    return 1 - compute_cosineSimilarity(u,v)\n",
    "\n",
    "print(listSortedKeys[:20])\n",
    "for word in listSortedKeys[:20]:\n",
    "    index = dictVocab2idx[word]\n",
    "    wordEmbedding = embedding[index]\n",
    "    listNearestNeighbors = []\n",
    "    for i,e in enumerate(embedding):\n",
    "        if compute_cosineDistance(wordEmbedding,e) < 0.47:\n",
    "            listNearestNeighbors.append(dictIdx2Vocab[i])\n",
    "    if len(listNearestNeighbors) > 1:\n",
    "        print(listNearestNeighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbors which make perfect sense :\n",
    "- would, could, may, also : these usually appear together\n",
    "- year, month\n",
    "- two, three\n",
    "- day, week\n",
    "\n",
    "For the most part, it appears that the similar words are closer together in the embedding space which means that the word meanings are captured reasonably well.\n",
    "\n",
    "However, things are not perfect though, for eg, seemingly unrelated words like first, man, day appear to be very close together. Perhaps, a better distance metric can provide a clearer picture of the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Collaborative Filtering (50 points)\n",
    "\n",
    "In this second part, you will implement collaborative filtering on the Netflix prize dataset -- donâ€™t freak out, the provided sample dataset has only ~2000 items and ~28,000 users.\n",
    "\n",
    "As background, read the paper [Empirical Analysis of Predictive Algorithms for Collaborative Filtering](https://arxiv.org/pdf/1301.7363.pdf) up to Section 2.1. Of course you can read further if you are interested, and you can also refer to the course slides for collaborative filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Netflix Data\n",
    "\n",
    "The dataset is subset of movie ratings data from the Netflix Prize Challenge. Download the dataset from Piazza. It contains a train set, test set, movie file, and README file. The last two files are original ones from the Netflix Prize, however; in this homework you will deal with train and test files which both are subsets of the Netflix training data. Each of train and test files has lines having this format: MovieID,UserID,Rating.\n",
    "\n",
    "Your job is to predict a rating in the test set using those provided in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of netflix-dataset/TrainingRatings.txt\n",
      "Number of ratings : 3255352\n",
      "Number of users : 28978\n",
      "Number of movies : 1821\n",
      "\n",
      "Summary of netflix-dataset/TestingRatings.txt\n",
      "Number of ratings : 100478\n",
      "Number of users : 27555\n",
      "Number of movies : 1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data, then print out the number of ratings, \n",
    "# movies and users in each of train and test sets.\n",
    "def create_utilityMatrix(ratings_file):\n",
    "    \"\"\"\n",
    "    Computes the total number of unique users and items and returns the values\n",
    "    \"\"\"\n",
    "    #The lines are in the format : movieId,userId,rating\n",
    "    dictMovies2Index = {}\n",
    "    dictUsers2Index = {}\n",
    "    numRatings = 0\n",
    "    movieIndex = 0\n",
    "    userIndex = 0\n",
    "    with open(ratings_file) as file:\n",
    "        for line in file:\n",
    "            movieId,userId,rating = line.split(',')\n",
    "            numRatings += 1\n",
    "            if userId not in dictUsers2Index:\n",
    "                dictUsers2Index[userId] = userIndex\n",
    "                userIndex += 1\n",
    "            if movieId not in dictMovies2Index:\n",
    "                dictMovies2Index[movieId] = movieIndex\n",
    "                movieIndex += 1\n",
    "    numUsers = userIndex\n",
    "    numMovies = movieIndex\n",
    "    print('Summary of {}'.format(ratings_file))\n",
    "    print('Number of ratings : {}'.format(numRatings))\n",
    "    print('Number of users : {}'.format(numUsers))\n",
    "    print('Number of movies : {}\\n'.format(numMovies))\n",
    "\n",
    "    utility_matrix = np.zeros(shape=(numUsers,numMovies))\n",
    "    with open(ratings_file) as file:\n",
    "        for line in file:\n",
    "            movieId,userId,rating = line.split(',')\n",
    "            user = dictUsers2Index[userId]\n",
    "            movie = dictMovies2Index[movieId]\n",
    "            utility_matrix[user][movie] = rating\n",
    "    return utility_matrix, dictUsers2Index, dictMovies2Index\n",
    "\n",
    "training_file = 'netflix-dataset/TrainingRatings.txt'\n",
    "testing_file = 'netflix-dataset/TestingRatings.txt'\n",
    "utility_matrix, dictUsers2Index, dictMovies2Index = create_utilityMatrix(training_file)\n",
    "_ = create_utilityMatrix(testing_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implement CF\n",
    "\n",
    "In this part, you will implement the basic collaborative filtering algorithm described in Section 2.1 of the paper -- that is, focus only on Equations 1 and 2 (where Equation 2 is just the Pearson correlation). You should consider the first 5,000 users with their associated items in the test set. \n",
    "\n",
    "Note that you should test the algorithm for a small set of users e.g., 10 users first and then run for 5,000 users. It may take long to run but you won't have memory issues. \n",
    "\n",
    "Set k to 0.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the right value for k proved to be to tedious and I simply could not find a suitable value. So, instead I used the following equation :\n",
    "\n",
    "**p<sub>a,j</sub> = avg(v<sub>a</sub>) + avg(v<sub>i,j</sub> - mean(v<sub>i</sub>))**\n",
    "\n",
    "**v<sub>i,j</sub> - mean(v<sub>i</sub>)** represents the net preference of user i for item j w.r.t her mean rating value.\n",
    "\n",
    "The idea is to correct the user average by collective preference of the similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(utility_matrix, dictUsers2Index, dictMovies2Index, testing_file):\n",
    "    \"\"\"\n",
    "    For every user,movie pair in Testing set, find the set of the most similar users in\n",
    "    the training set who have rated movie and compute the average rating\n",
    "    \"\"\"\n",
    "    def get_commonRatings(ratings, mask):\n",
    "        common_ratings = ratings * mask\n",
    "        # Remove the zero elements, these lead to a higher correlation value\n",
    "        # than the reality\n",
    "        return common_ratings[common_ratings != 0]\n",
    "\n",
    "    predictions = []\n",
    "    expected_values = []\n",
    "    import sys\n",
    "    sys.stdout.flush()\n",
    "    with open(testing_file) as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip() # Remove leading/trailing whitespace\n",
    "            movieId, userId, rating = stripped_line.split(',')\n",
    "            expected_values.append(float(rating))\n",
    "            print(\"predicting {}'s rating for {}\".format(userId,movieId))\n",
    "            activeUserIndex = dictUsers2Index[userId]\n",
    "            activeMovieIndex = dictMovies2Index[movieId]\n",
    "            # Create a mask for the active user, indicating the rated items\n",
    "            ratings_activeUser = utility_matrix[activeUserIndex]\n",
    "            mask = ratings_activeUser != 0\n",
    "            numSimilarUsers = 0\n",
    "            sum_similarUserRatings = 0\n",
    "            # Initialize the predicted rating with the average rating value for\n",
    "            # the active user, later this is adjusted with the collaborative\n",
    "            # value\n",
    "            pred_rating = ratings_activeUser[ratings_activeUser != 0].mean()\n",
    "            for ratings_user in utility_matrix:\n",
    "                if ratings_user[activeMovieIndex] != 0:\n",
    "                    # To measure the similarity, we need to only compare the\n",
    "                    # items rated by both users, so create a mask of common\n",
    "                    # items\n",
    "                    currUser_mask = mask * (ratings_user != 0)\n",
    "                    # if the mask contains only 2 items, pearson correlation\n",
    "                    # always returns 1 which is not very useful\n",
    "                    if (sum(currUser_mask) > 2):\n",
    "                        # Get the ratings of the common items\n",
    "                        user_commonRatings = get_commonRatings(\n",
    "                            ratings_user, currUser_mask)\n",
    "                        activeUser_commonRatings = get_commonRatings(\n",
    "                            ratings_activeUser, currUser_mask)\n",
    "                        pearson_correlation = pearsonr(activeUser_commonRatings, user_commonRatings)[0]\n",
    "                      #  print(user_commonRatings, activeUser_commonRatings,\n",
    "                      #        pearson_correlation)\n",
    "                        if pearson_correlation > 0.7:\n",
    "                            print(\"active user and similar user's rating : {} and {}\".format(activeUser_commonRatings, user_commonRatings))\n",
    "                            numSimilarUsers += 1\n",
    "                            sum_similarUserRatings += ( ratings_user[activeMovieIndex] - user_commonRatings.mean())\n",
    "            pred_rating += (sum_similarUserRatings / (numSimilarUsers + 0.0001))\n",
    "            predictions.append(pred_rating)\n",
    "        predictions = np.array(predictions)\n",
    "        expected_values = np.array(expected_values)\n",
    "        print(predictions, expected_values)\n",
    "        return predictions, expected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation \n",
    "\n",
    "You should evaluate your predictions using Mean Absolute Error and Root Mean Squared Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'netflix-dataset/TrainingRatings.txt'\n",
    "testing_file = 'netflix-dataset/TestingRatings_small.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(predictions, expected_values):\n",
    "    return (np.absolute(predictions - expected_values)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(predictions, expected_values):\n",
    "    return np.sqrt(((predictions - expected_values) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the model for the first 600 users in the Testing Set and got the following results:  \n",
    "**rmse : 1.092**  \n",
    "**mae  : 0.852**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Extensions\n",
    "\n",
    "Given your results in the previous part, can you do better? For this last part you should report on your best attempt at improving MAE and RMSE. Provide code, results, plus a brief discussion on your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried using cosine similarity with mean subtraction instead of pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosineSimilarity(a,b):\n",
    "    return np.dot(a,b)/ (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def compute_similarity(u, v):\n",
    "    u_m = u[u != 0].mean()\n",
    "    v_m = v[v != 0].mean()\n",
    "    u_mask = (u != 0) * u_m\n",
    "    v_mask = (v != 0) * v_m\n",
    "    return compute_cosineSimilarity(u - u_mask, v - v_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the following results :  \n",
    "**rmse : 1.092**  \n",
    "**mae  : 0.852**  \n",
    "This is exactly the same as before which, in retrospect, is unsurprising since cosine similarity with mean subtraction is the same as pearson correlation!  \n",
    "Nonetheless, this was my best attempt at improving RMSE and all other attempts ended up worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
